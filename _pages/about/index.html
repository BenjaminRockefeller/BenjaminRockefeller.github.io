<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="-technical-focus">🔷 Technical Focus</h2> <p>What sets me apart is a rare blend of deep LLM engineering expertise and real-world product leadership.</p> <p>With over 6 years of experience spanning product design, strategic execution, cross-functional delivery, and digital transformation, I understand not only how to build large-scale AI systems—but why they matter, who they serve, and what it takes to make them operate reliably in high-stakes environments.</p> <p>I may not be the strongest coder—but I’m one of the few LLM engineers who can consistently translate abstract models into resilient, user-grounded, and business-aligned solutions—built for ambiguity, deployed under constraints, and designed for scale.</p> <p>I don’t just build for performance.<br> I build for purpose—and I deliver where it counts.</p> <ul> <li>Fine-tuning &amp; Adaptation: LoRA, QLoRA, multi-task learning, continual training</li> <li>Retrieval &amp; Reasoning: RAG, tool-calling, context-aware QA, long-document reasoning</li> <li>Reinforcement Learning: PPO, DPO, RLAIF, reward modeling, alignment tuning</li> <li>AI Agents: LangGraph, AutoGen, agent orchestration, task routing, feedback loops</li> <li>Memory &amp; State (MCP): Model Context Protocol design for long-range multi-agent memory</li> <li>Inference Optimization: vLLM, LMDeploy, INT8 quantization, GPU/CPU hybrid scheduling</li> <li>System Engineering: tokenizer design, model versioning, CI/CD pipelines, deployment infra</li> <li>Robustness &amp; Evaluation: prompt stress tests, hallucination benchmarks, intent-grounded metrics</li> </ul> <hr> <h2 id="-research-interest">🔷 Research Interest</h2> <p><strong>From fluency to trust: how do we build reasoning-capable LLMs?</strong></p> <p>My research explores how to move large language models (LLMs) beyond surface fluency—toward dependable, reflective, and cognitively aligned reasoning.</p> <p>Today’s models speak well—but they hallucinate, forget, and fail to explain themselves. This isn’t just a technical flaw—it’s a cognitive gap that threatens how humans trust machines.</p> <p>To solve this, I focus on closed-loop generation—designing models that reflect, revise, and adapt. Key systems include:</p> <ul> <li>🌀 <strong>REFLEXION</strong>: Multi-step feedback loops embedded in Transformers, reducing hallucinations and improving interpretability.</li> <li>🔍 <strong>CAT (Citation-Aware Tagging)</strong>: Anchors generations to verifiable sources, enabling traceability and epistemic integrity.</li> <li>🧠 <strong>MEMOFORMER</strong>: Adds long-term memory across documents, powering multi-context reasoning over extended timelines.</li> </ul> <p>I also explore multimodal cognition and nonlinear inference—enabling models to self-interrogate, reason across text/image/data, and dynamically adapt to ambiguity.</p> <p>I don’t just study how LLMs generate language—<strong>I build systems that reason, reflect, and earn trust.</strong></p> </body></html>