---
layout: about
title: Start Here
permalink: /
subtitle: LLM Engineer | AI Researcher | Economics-Driven Systems Thinker from Nanyang Technological University

profile:
  align: right
  image: ben.jpg
  image_circular: true
  image_style: "margin-top: -30px;"  # 将头像整体上移，贴近标题
  more_info: >
    <div style="margin-top: 1.2em; font-size: 17px; line-height: 1.6;">
      <p><strong>🎓 MSc in Applied Economics</strong><br>Nanyang Technological University</p>
      <p><strong>🎓 BA in International Economics and Trade</strong><br>Nankai University</p>
      <p><strong>📬</strong> <a href="mailto:jianming001@e.ntu.edu.sg">jianming001@e.ntu.edu.sg</a></p>
      <p><strong>🔗</strong> <a href="https://linkedin.com/in/benjaminrockefeller" target="_blank">LinkedIn</a></p>
    </div>

selected_papers: false
social: false
announcements:
  enabled: false
latest_posts:
  enabled: false
---

## 🔷 Research Interest

**From fluency to trust: how do we build reasoning-capable LLMs?**

My research explores how to move large language models (LLMs) beyond surface fluency—toward dependable, reflective, and cognitively aligned reasoning.

Today’s models speak well—but they hallucinate, forget, and fail to explain themselves. This isn’t just a technical flaw—it’s a cognitive gap that threatens how humans trust machines.

To solve this, I focus on closed-loop generation—designing models that reflect, revise, and adapt. Key systems include:

- 🌀 **REFLEXION**: Multi-step feedback loops embedded in Transformers, reducing hallucinations and improving interpretability.  
- 🔍 **CAT (Citation-Aware Tagging)**: Anchors generations to verifiable sources, enabling traceability and epistemic integrity.  
- 🧠 **MEMOFORMER**: Adds long-term memory across documents, powering multi-context reasoning over extended timelines.  

I also explore multimodal cognition and nonlinear inference—enabling models to self-interrogate, reason across text/image/data, and dynamically adapt to ambiguity.

I don’t just study how LLMs generate language—**I build systems that reason, reflect, and earn trust.**

---

## 🔷 Technical Focus

What sets me apart is a rare blend of deep LLM engineering expertise and real-world product leadership.

With over 6 years of experience spanning product design, strategic execution, cross-functional delivery, and digital transformation, I understand not only how to build large-scale AI systems—but why they matter, who they serve, and what it takes to make them operate reliably in high-stakes environments.

I may not be the strongest coder—but I’m one of the few LLM engineers who can consistently translate abstract models into resilient, user-grounded, and business-aligned solutions—built for ambiguity, deployed under constraints, and designed for scale.

I don’t just build for performance.  
I build for purpose—and I deliver where it counts.

- Fine-tuning & Adaptation: LoRA, QLoRA, multi-task learning, continual training  
- Retrieval & Reasoning: RAG, tool-calling, context-aware QA, long-document reasoning  
- Reinforcement Learning: PPO, DPO, RLAIF, reward modeling, alignment tuning  
- AI Agents: LangGraph, AutoGen, agent orchestration, task routing, feedback loops  
- Memory & State (MCP): Model Context Protocol design for long-range multi-agent memory  
- Inference Optimization: vLLM, LMDeploy, INT8 quantization, GPU/CPU hybrid scheduling  
- System Engineering: tokenizer design, model versioning, CI/CD pipelines, deployment infra  
- Robustness & Evaluation: prompt stress tests, hallucination benchmarks, intent-grounded metrics  

---
