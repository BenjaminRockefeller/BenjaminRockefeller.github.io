---
layout: about
title: Start Here
permalink: /
subtitle: AI Researcher·Nanyang Technological University, Singapore 

profile:
  align: right
  image: ben.jpg
  image_circular: true
  more_info: >
    <p>Nanyang Technological University</p>
    <p>Singapore</p>
    <p>jianming001@e.ntu.edu.sg</p>
    
selected_papers: false  
social: true 

announcements:
  enabled: false  
 

latest_posts:
  enabled: false  
  
---
## Benjamin L. Rockefeller (Lai Jianming)

🎓 **Nanyang Technological University**  
MSc in Applied Economics

🎓 **Nankai University**  
BA in International Economics and Trade

📧 **Email**: [jianming001@e.ntu.edu.sg](mailto:jianming001@e.ntu.edu.sg)  
🔗 **LinkedIn**: [linkedin.com/in/benjaminrockefeller](https://linkedin.com/in/benjaminrockefeller)

---

## 🚀 LLM Engineer · AI Researcher · Economics-Driven Systems Thinker

---

## 🔷 Rewired by AI

Not a trend, not a tool—AI rebuilt how I think, learn, and live.  
I didn’t choose artificial intelligence because it was popular—I chose it because it changed me when I needed it most.

At my lowest point, I was trapped in self-doubt and existential confusion, my mind full of unrest. Ideas came constantly, but they felt unreachable—separated from action by an unbridgeable gap.

Then I encountered AI. It opened a new path—not just a toolset, but a restructuring of how I think.  
It became a way forward—a framework for continuous growth and self-transcendence.

To me, AI is not just a technology—it’s a cognitive revolution.  
It gave me the ability to turn abstract visions into grounded systems.

I chose AI not because it trends, but because it helps me break through mental limits and build intelligent systems that think, adapt, and evolve.  
AI changed me—and now, I choose to evolve with it.

---

## 🔷 Technical Focus

What sets me apart is a rare blend of deep LLM engineering expertise and real-world product leadership.

With over 6 years of experience spanning product design, strategic execution, cross-functional delivery, and digital transformation, I understand not only how to build large-scale AI systems—but why they matter, who they serve, and what it takes to make them operate reliably in high-stakes environments.

I may not be the strongest coder—but I’m one of the few LLM engineers who can consistently translate abstract models into resilient, user-grounded, and business-aligned solutions—built for ambiguity, deployed under constraints, and designed for scale.

I don’t just build for performance.  
I build for purpose—and I deliver where it counts.

- Fine-tuning & Adaptation: LoRA, QLoRA, multi-task learning, continual training  
- Retrieval & Reasoning: RAG, tool-calling, context-aware QA, long-document reasoning  
- Reinforcement Learning: PPO, DPO, RLAIF, reward modeling, alignment tuning  
- AI Agents: LangGraph, AutoGen, agent orchestration, task routing, feedback loops  
- Memory & State (MCP): Model Context Protocol design for long-range multi-agent memory  
- Inference Optimization: vLLM, LMDeploy, INT8 quantization, GPU/CPU hybrid scheduling  
- System Engineering: tokenizer design, model versioning, CI/CD pipelines, deployment infra  
- Robustness & Evaluation: prompt stress tests, hallucination benchmarks, intent-grounded metrics  

---

## 🔷 Research Interest

**From fluency to trust: how do we build reasoning-capable LLMs?**

My research explores how to move large language models (LLMs) beyond surface fluency—toward dependable, reflective, and cognitively aligned reasoning.

Today’s models speak well—but they hallucinate, forget, and fail to explain themselves. This isn’t just a technical flaw—it’s a cognitive gap that threatens how humans trust machines.

To solve this, I focus on closed-loop generation—designing models that reflect, revise, and adapt. Key systems include:

- 🌀 **REFLEXION**: Multi-step feedback loops embedded in Transformers, reducing hallucinations and improving interpretability.  
- 🔍 **CAT (Citation-Aware Tagging)**: Anchors generations to verifiable sources, enabling traceability and epistemic integrity.  
- 🧠 **MEMOFORMER**: Adds long-term memory across documents, powering multi-context reasoning over extended timelines.  

I also explore multimodal cognition and nonlinear inference—enabling models to self-interrogate, reason across text/image/data, and dynamically adapt to ambiguity.

I don’t just study how LLMs generate language—**I build systems that reason, reflect, and earn trust.**
