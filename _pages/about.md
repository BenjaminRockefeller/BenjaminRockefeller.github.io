---
layout: about
title: Start Here
permalink: /
subtitle: LLM Engineer | AI Researcher | Economics-Driven Systems Thinker from Nanyang Technological University

profile:
  align: right
  image: ben.jpg
  image_circular: true
  image_style: "margin-top: -30px;"  # å°†å¤´åƒæ•´ä½“ä¸Šç§»ï¼Œè´´è¿‘æ ‡é¢˜
  more_info: >
    <div style="margin-top: 1.2em; font-size: 17px; line-height: 1.6;">
      <p><strong>ğŸ“ MSc in Applied Economics</strong><br>Nanyang Technological University</p>
      <p><strong>ğŸ“ BA in International Economics and Trade</strong><br>Nankai University</p>
      <p><strong>ğŸ“¬</strong> <a href="mailto:jianming001@e.ntu.edu.sg">jianming001@e.ntu.edu.sg</a></p>
      <p><strong>ğŸ”—</strong> <a href="https://linkedin.com/in/benjaminrockefeller" target="_blank">LinkedIn</a></p>
    </div>

selected_papers: false
social: false
announcements:
  enabled: false
latest_posts:
  enabled: false
---

## ğŸ”· Research Interest

**From fluency to trust: how do we build reasoning-capable LLMs?**

My research explores how to move large language models (LLMs) beyond surface fluencyâ€”toward dependable, reflective, and cognitively aligned reasoning.

Todayâ€™s models speak wellâ€”but they hallucinate, forget, and fail to explain themselves. This isnâ€™t just a technical flawâ€”itâ€™s a cognitive gap that threatens how humans trust machines.

To solve this, I focus on closed-loop generationâ€”designing models that reflect, revise, and adapt. Key systems include:

- ğŸŒ€ **REFLEXION**: Multi-step feedback loops embedded in Transformers, reducing hallucinations and improving interpretability.  
- ğŸ” **CAT (Citation-Aware Tagging)**: Anchors generations to verifiable sources, enabling traceability and epistemic integrity.  
- ğŸ§  **MEMOFORMER**: Adds long-term memory across documents, powering multi-context reasoning over extended timelines.  

I also explore multimodal cognition and nonlinear inferenceâ€”enabling models to self-interrogate, reason across text/image/data, and dynamically adapt to ambiguity.

I donâ€™t just study how LLMs generate languageâ€”**I build systems that reason, reflect, and earn trust.**

---

## ğŸ”· Technical Focus

What sets me apart is a rare blend of deep LLM engineering expertise and real-world product leadership.

With over 6 years of experience spanning product design, strategic execution, cross-functional delivery, and digital transformation, I understand not only how to build large-scale AI systemsâ€”but why they matter, who they serve, and what it takes to make them operate reliably in high-stakes environments.

I may not be the strongest coderâ€”but Iâ€™m one of the few LLM engineers who can consistently translate abstract models into resilient, user-grounded, and business-aligned solutionsâ€”built for ambiguity, deployed under constraints, and designed for scale.

I donâ€™t just build for performance.  
I build for purposeâ€”and I deliver where it counts.

- Fine-tuning & Adaptation: LoRA, QLoRA, multi-task learning, continual training  
- Retrieval & Reasoning: RAG, tool-calling, context-aware QA, long-document reasoning  
- Reinforcement Learning: PPO, DPO, RLAIF, reward modeling, alignment tuning  
- AI Agents: LangGraph, AutoGen, agent orchestration, task routing, feedback loops  
- Memory & State (MCP): Model Context Protocol design for long-range multi-agent memory  
- Inference Optimization: vLLM, LMDeploy, INT8 quantization, GPU/CPU hybrid scheduling  
- System Engineering: tokenizer design, model versioning, CI/CD pipelines, deployment infra  
- Robustness & Evaluation: prompt stress tests, hallucination benchmarks, intent-grounded metrics  

---
