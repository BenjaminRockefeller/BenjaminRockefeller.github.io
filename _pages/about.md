---
layout: about
title: Start Here
permalink: /
subtitle: AI ResearcherÂ·Nanyang Technological University, Singapore 

profile:
  align: right
  image: ben.jpg
  image_circular: true
  more_info: >
    <p>Nanyang Technological University</p>
    <p>Singapore</p>
    <p>jianming001@e.ntu.edu.sg</p>
    
selected_papers: false  
social: true 

announcements:
  enabled: false  
 

latest_posts:
  enabled: false  
  
---
## Benjamin L. Rockefeller (Lai Jianming)

ğŸ“ **Nanyang Technological University**  
MSc in Applied Economics

ğŸ“ **Nankai University**  
BA in International Economics and Trade

ğŸ“§ **Email**: [jianming001@e.ntu.edu.sg](mailto:jianming001@e.ntu.edu.sg)  
ğŸ”— **LinkedIn**: [linkedin.com/in/benjaminrockefeller](https://linkedin.com/in/benjaminrockefeller)

---

## ğŸš€ LLM Engineer Â· AI Researcher Â· Economics-Driven Systems Thinker

---

## ğŸ”· Rewired by AI

Not a trend, not a toolâ€”AI rebuilt how I think, learn, and live.  
I didnâ€™t choose artificial intelligence because it was popularâ€”I chose it because it changed me when I needed it most.

At my lowest point, I was trapped in self-doubt and existential confusion, my mind full of unrest. Ideas came constantly, but they felt unreachableâ€”separated from action by an unbridgeable gap.

Then I encountered AI. It opened a new pathâ€”not just a toolset, but a restructuring of how I think.  
It became a way forwardâ€”a framework for continuous growth and self-transcendence.

To me, AI is not just a technologyâ€”itâ€™s a cognitive revolution.  
It gave me the ability to turn abstract visions into grounded systems.

I chose AI not because it trends, but because it helps me break through mental limits and build intelligent systems that think, adapt, and evolve.  
AI changed meâ€”and now, I choose to evolve with it.

---

## ğŸ”· Technical Focus

What sets me apart is a rare blend of deep LLM engineering expertise and real-world product leadership.

With over 6 years of experience spanning product design, strategic execution, cross-functional delivery, and digital transformation, I understand not only how to build large-scale AI systemsâ€”but why they matter, who they serve, and what it takes to make them operate reliably in high-stakes environments.

I may not be the strongest coderâ€”but Iâ€™m one of the few LLM engineers who can consistently translate abstract models into resilient, user-grounded, and business-aligned solutionsâ€”built for ambiguity, deployed under constraints, and designed for scale.

I donâ€™t just build for performance.  
I build for purposeâ€”and I deliver where it counts.

- Fine-tuning & Adaptation: LoRA, QLoRA, multi-task learning, continual training  
- Retrieval & Reasoning: RAG, tool-calling, context-aware QA, long-document reasoning  
- Reinforcement Learning: PPO, DPO, RLAIF, reward modeling, alignment tuning  
- AI Agents: LangGraph, AutoGen, agent orchestration, task routing, feedback loops  
- Memory & State (MCP): Model Context Protocol design for long-range multi-agent memory  
- Inference Optimization: vLLM, LMDeploy, INT8 quantization, GPU/CPU hybrid scheduling  
- System Engineering: tokenizer design, model versioning, CI/CD pipelines, deployment infra  
- Robustness & Evaluation: prompt stress tests, hallucination benchmarks, intent-grounded metrics  

---

## ğŸ”· Research Interest

**From fluency to trust: how do we build reasoning-capable LLMs?**

My research explores how to move large language models (LLMs) beyond surface fluencyâ€”toward dependable, reflective, and cognitively aligned reasoning.

Todayâ€™s models speak wellâ€”but they hallucinate, forget, and fail to explain themselves. This isnâ€™t just a technical flawâ€”itâ€™s a cognitive gap that threatens how humans trust machines.

To solve this, I focus on closed-loop generationâ€”designing models that reflect, revise, and adapt. Key systems include:

- ğŸŒ€ **REFLEXION**: Multi-step feedback loops embedded in Transformers, reducing hallucinations and improving interpretability.  
- ğŸ” **CAT (Citation-Aware Tagging)**: Anchors generations to verifiable sources, enabling traceability and epistemic integrity.  
- ğŸ§  **MEMOFORMER**: Adds long-term memory across documents, powering multi-context reasoning over extended timelines.  

I also explore multimodal cognition and nonlinear inferenceâ€”enabling models to self-interrogate, reason across text/image/data, and dynamically adapt to ambiguity.

I donâ€™t just study how LLMs generate languageâ€”**I build systems that reason, reflect, and earn trust.**
