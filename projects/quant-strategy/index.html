<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="-project-2intelligent-quantitative-strategy-generation-platform">✅ Project 2｜Intelligent Quantitative Strategy Generation Platform</h2> <h3 id="causality-aware-and-rlhf-based-backtesting-optimization-system">Causality-Aware and RLHF-Based Backtesting Optimization System</h3> <p><em>(Supporting Closed-Loop Strategy Generation, Multi-Task Distillation, and Reinforcement Learning with Backtest Feedback)</em></p> <hr> <h4 id="project-overview">Project Overview</h4> <p>This project tackles persistent pain points in quant strategy development: unverifiable outputs, missing causal structures, and high deployment cost. It proposes a closed-loop generation system unifying strategy generation, causal graph extraction, RLHF optimization, and backtesting feedback into a production-ready pipeline.</p> <hr> <h4 id="project-contributions">Project Contributions</h4> <ul> <li>Constructed a high-quality multi-source corpus of 800K+ items, spanning: <ol> <li>12,000 strategy summaries</li> <li>11,000 indicator chains</li> <li>9,000 causal market graphs</li> </ol> </li> <li> <p>Resolved noisy/heterogeneous inputs with rule-based extraction (spaCy + tree parser) and entity alignment via graph databases. Structural consistency improved from 71.2% → 94.6%.</p> </li> <li> <p>Designed a multi-task distillation system with LLaMA‑2‑7B (teacher); trained with soft-labels (T=2.0) and balanced 3 loss functions (0.4:0.4:0.2) to support generalization across tasks.</p> </li> <li>Created a 3-dimensional reward function for RLHF: <ol> <li><strong>Causal Coverage</strong></li> <li><strong>Contextual Coherence</strong></li> <li> <strong>Backtest Return Stability</strong><br> Implemented reward clipping, early baseline, and dynamic weighting to prevent collapse.</li> </ol> </li> <li>Introduced a full backtest feedback loop: <ul> <li>Generated strategies converted to Python scripts</li> <li>Simulated via Backtrader</li> <li>Return, Sharpe, Drawdown metrics used as third RLHF reward<br> → Forming a <strong>generate → simulate → retrain</strong> loop.</li> </ul> </li> <li>Deployed on RTX 3090 with QLoRA + TensorRT INT4 compression (−65% size); latency = 270ms, throughput = 26 req/s.</li> </ul> <hr> <h4 id="experimental-results">Experimental Results</h4> <table> <thead> <tr> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>BLEU Improvement</td> <td>+14.6%</td> </tr> <tr> <td>Redundancy Reduction</td> <td>−21.2%</td> </tr> <tr> <td>Causal Path Accuracy</td> <td>85.4%</td> </tr> <tr> <td>Strategy Simulation Success</td> <td>78.9%</td> </tr> <tr> <td>Expert Rating</td> <td>4.7 / 5.0</td> </tr> <tr> <td>RLHF Convergence Acceleration</td> <td>+32%</td> </tr> </tbody> </table> <hr> <h4 id="optimization--future-directions">Optimization &amp; Future Directions</h4> <ul> <li> <strong>Ablation Study</strong>: Built 3 variants (no causal / no RL / no backtest) to analyze contribution paths.</li> <li> <strong>Visualization Tools</strong>: D3.js + Plotly visualizations of causal graphs, returns, prompt-generation flow.</li> <li> <strong>Model Portability</strong>: Adapted to InternLM, Mistral-7B, Yi-6B with &lt;2.7% consistency error.</li> <li> <strong>Self-Adaptive Loop</strong>: Introduced Q-learning &amp; R2D2 to rewrite prompts dynamically when return &lt; threshold.</li> </ul> <hr> <h4 id="tech-stack">Tech Stack</h4> <p>LLaMA‑2‑7B, InternLM, QLoRA, LoRA Adapter, RLHF, Soft Label Distillation, Backtrader, GraphDB, Prompt Chain System, FastAPI, Docker, PyTorch Lightning, WandB, TensorRT INT4, JSON/XML/GraphML output, RTX 3090.</p> <hr> </body></html>